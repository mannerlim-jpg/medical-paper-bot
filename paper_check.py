import requests
import datetime
import os
import xml.etree.ElementTree as ET
import re

# 1. ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì„¤ì •
CATEGORIES = {
    "ğŸ¤– ë¡œë´‡ ë³´ì¡° ìˆ˜ìˆ  (Robot-Assisted)": ["ROBOT", "MAKO", "NAVIO", "ROSA", "NAVIGAT"],
    "ğŸ¦¶ ë°œëª© ë° ì¡±ë¶€ (Ankle & Foot)": ["ANKLE", "TALAR", "ACHILLES", "FOOT"],
    "ğŸ¦µ ë¬´ë¦ ë° ì¼ë°˜ ê´€ì ˆê²½ (Knee & General)": ["KNEE", "TKR", "TKA", "ARTHROSCOP", "ACL"]
}

def fetch_papers():
    today = datetime.date.today()
    seen_links = set()
    
    # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ ê¸°ì¡´ ë§í¬ ì½ê¸°
    if os.path.exists("papers.md"):
        with open("papers.md", "r", encoding="utf-8") as f:
            seen_links = set(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', f.read()))

    # arXiv API ê²€ìƒ‰ (ëª¨ë“  í‚¤ì›Œë“œ í†µí•© ê²€ìƒ‰)
    all_keywords = [k for v in CATEGORIES.values() for k in v]
    query = " OR ".join([f'all:"{k}"' for k in all_keywords])
    url = f"http://export.arxiv.org/api/query?search_query={query}&sortOrder=descending&max_results=30"
    
    response = requests.get(url)
    root = ET.fromstring(response.text)
    
    # ë¶„ë¥˜ë¥¼ ìœ„í•œ ë°”êµ¬ë‹ˆ(Dictionary) ì¤€ë¹„
    classified_report = {cat: [] for cat in CATEGORIES.keys()}

    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
        title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip().replace('\n', ' ')
        link = entry.find('{http://www.w3.org/2005/Atom}id').text.strip()
        
        if link in seen_links:
            continue

        # ì œëª© ë¶„ì„ í›„ ì¹´í…Œê³ ë¦¬ ë°°ì •
        title_upper = title.upper()
        assigned = False
        for cat, keywords in CATEGORIES.items():
            if any(k in title_upper for k in keywords):
                classified_report[cat].append({"title": title, "link": link})
                assigned = True
                break # í•œ ë…¼ë¬¸ì€ í•˜ë‚˜ì˜ ì¹´í…Œê³ ë¦¬ì—ë§Œ ë°°ì •
        
    # ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ íŒŒì¼ì— ê¸°ë¡ ë° ë³´ê³ ì„œ ì‘ì„±
    has_new_content = any(len(papers) > 0 for papers in classified_report.values())

    if has_new_content:
        with open("papers.md", "a", encoding="utf-8") as f:
            f.write(f"\n\n## ğŸ“… {today} ì§€ëŠ¥í˜• ë…¼ë¬¸ ë¶„ë¥˜ ë³´ê³ \n")
            for cat, papers in classified_report.items():
                if papers: # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ì‹ ê·œ ë…¼ë¬¸ì´ ìˆì„ ë•Œë§Œ ì„¹ì…˜ ìƒì„±
                    f.write(f"\n### {cat}\n")
                    for p in papers:
                        f.write(f"* **ì œëª©:** {p['title']}\n  * **ë§í¬:** {p['link']}\n")
        print("ì‹ ê·œ ë…¼ë¬¸ ë¶„ë¥˜ ì™„ë£Œ!")
    else:
        print("ì˜¤ëŠ˜ ì¶”ê°€ëœ ì‹ ê·œ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    fetch_papers()
