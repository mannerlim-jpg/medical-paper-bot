import requests
import datetime
import os
import xml.etree.ElementTree as ET
import re

# 1. ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ - ë¡œë´‡ ë³´ì¡° TKRì— ì§‘ì¤‘í•˜ë„ë¡ ìˆ˜ì •
CATEGORIES = {
    "ğŸ¤– ë¡œë´‡ ë³´ì¡° TKR (Robot-Assisted TKR)": ["ROBOT ASSISTED TKR", "ROBOT ASSISTED TKA", "ROBOTIC TKR", "ROBOTIC TKA", "ROBOTIC TOTAL KNEE"],
    "ğŸ¦µ ì¼ë°˜ TKR ë° ì¸ê³µê´€ì ˆ (General TKR)": ["TOTAL KNEE REPLACEMENT", "TOTAL KNEE ARTHROPLASTY", "TKR", "TKA"],
    "ğŸ” ë¬´ë¦ ë° ë°œëª© ê´€ì ˆê²½ (Arthroscopy)": ["KNEE ARTHROSCOPY", "ANKLE ARTHROSCOPY", "ARTHROSCOPIC SURGERY"]
}

def fetch_papers():
    today = datetime.date.today()
    seen_links = set()
    
    # ì¤‘ë³µ ì²´í¬
    if os.path.exists("papers.md"):
        with open("papers.md", "r", encoding="utf-8") as f:
            seen_links = set(re.findall(r'https?://arxiv\.org/abs/\S+', f.read()))

    # arXiv API ê²€ìƒ‰
    all_search_terms = [k for v in CATEGORIES.values() for k in v]
    query = " OR ".join([f'all:"{k}"' for k in all_search_terms])
    url = f"https://export.arxiv.org/api/query?search_query={query}&sortOrder=descending&max_results=50"
    
    response = requests.get(url)
    root = ET.fromstring(response.text)
    
    classified_report = {cat: [] for cat in CATEGORIES.keys()}

    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
        title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip().replace('\n', ' ')
        link = entry.find('{http://www.w3.org/2005/Atom}id').text.strip().replace('http://', 'https://')
        
        if link in seen_links: continue

        title_upper = title.upper()
        
        # ì¹´í…Œê³ ë¦¬ ë°°ì • ë¡œì§
        for cat, keywords in CATEGORIES.items():
            if any(k in title_upper for k in keywords):
                # ğŸ¤– ë¡œë´‡ ì¹´í…Œê³ ë¦¬ì˜ ê²½ìš° 'KNEE'ë‚˜ 'TKR/TKA'ê°€ ì œëª©ì— ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•¨ (ì´ì¤‘ í•„í„°)
                if "ğŸ¤–" in cat:
                    if not any(target in title_upper for target in ["KNEE", "TKR", "TKA"]):
                        continue
                
                if len(classified_report[cat]) < 5:
                    classified_report[cat].append({"title": title, "link": link})
                break
        
    has_new_content = any(len(papers) > 0 for papers in classified_report.values())

    if has_new_content:
        with open("papers.md", "a", encoding="utf-8") as f:
            f.write(f"\n\n## ğŸ“… {today} ì‹ ê·œ ë…¼ë¬¸ ë¸Œë¦¬í•‘\n")
            for cat, papers in classified_report.items():
                if papers:
                    f.write(f"\n### {cat}\n")
                    for p in papers:
                        # ë§í¬ë¥¼ ê´„í˜¸ ì—†ì´ Raw URLë¡œ ì œê³µí•˜ì—¬ í´ë¦­ ë¬¸ì œ í•´ê²°
                        f.write(f"* **ì œëª©:** {p['title']}\n  * **ì›ë¬¸:** {p['link']}\n")
        print("í•„í„°ë§ ë° ë¶„ë¥˜ ì™„ë£Œ!")
    else:
        print("ìƒˆë¡œ ì¶”ê°€í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    fetch_papers()
